---
title: Access to public transport
subtitle: With UNStats and other data
format:
  360-analysis-html: default
author: James Goldie
date: last-modified
code-fold: true
---

```{r}
#| label: setup
library(tidyverse)
library(countrycode)
library(fuzzyjoin)
library(httr2)
library(rvest)
library(here)
```

# Acquiring the data from UNStats

The UN's [SDG Data Portal](https://unstats.un.org/sdgs/dataportal/database) has data for the series that make up the indicators that track the goals. We'll be pulling data for indicator 11.2.1: percentage of the population with convenient access to public transport.

Although you can download this data from their site manually, here we'll see using their API.

If you need to test or look up endpoints, the API's [Swagger](https://unstats.un.org/sdgs/UNSDGAPIV5/swagger) is useful.

First we'll look up the name of the series tied to the indicator:

```{r}
#| label: lookup-series
api_root <- "https://unstats.un.org/sdgs/UNSDGAPIV5"

api_root |>
  paste0("/v1/sdg/Indicator/11.2.1/Series/List") |>
  request() |>
  req_perform() |>
  httr2::resp_body_json() ->
req_series_list

stopifnot("SDG indicator lookup should only return 1 result" =
  length(req_series_list) == 1)

req_series_list |>
  pluck(1, "series", 1, "code") ->
pt_series_name

stopifnot("SDG indicator lookup did not return a valid series code" =
  !is.null(pt_series_name))
```

Now we can get the data for the located series. Normally we'd use the endpoint `/v1/sdg/Series/Data`, supplying the `seriesCode` as a query string parameter. But I'm finding that that's dropping the city column (or, rather, leaving all the cities blank).

I'm not really sure why that is, but a workaround is to get each country's data individually using `/v1/sdg/Series/{seriesCode}/GeoArea/{geoAreaCode}/DataSlice`, where `geoAreaCode` is a numeric code corresponding to the country.

So first we need a list of countries with available data for this series. We can get that from `/v1/sdg/Series/{seriesCode}/GeoAreas`.

```{r}
#| label: get-areas
api_root |>
  paste0("/v1/sdg/Series/", pt_series_name, "/GeoAreas") |>
  request() |>
  req_perform() |>
  httr2::resp_body_json(simplifyVector = TRUE) ->
pt_countries
```

This is a data frame of `geoAreaName` (country names) and `geoAreaCode` (numeric codes for each one). We can use the codes to request PT data, one country at a time, and get the cities along for the ride.

```{r}
#| label: get-each-country
api_root |>
  paste0(
    "/v1/sdg/Series/SP_TRN_PUBL/GeoArea/",
    pt_countries[["geoAreaCode"]],
    "/DataSlice") |>
  map(request) |>
  map(req_throttle, rate = 1 / 2) |>
  req_perform_sequential() |>
  map(resp_body_json, simplifyVector = TRUE) |>
  bind_rows() |>
  unpack(dimensions) ->
pt_data
```

Virtually all of the data is from 2019, but a few are from earlier or later years:

```{r}
#| label: reporting-year
pt_data |> count(timePeriodStart) |> arrange(timePeriodStart)
```

Let's write it out to disk and map it in OJS. First, let's tidy up the city names, which appear to have ISO2 country codes prefixed to them.

A few countries also have additional data that needs to come out of the city names:

- Canada, China and (for some cities) the US have a state name or abbreviation at the end after a final underscore
- Mexico has a state abbreviation at the _start_ before an underscore
- Pakistan has three state or province names that come before some cities: `Khyber_pakhtunkhwa`, `Balochistan`, `Sindh` and `Punjab`. They also have totals for these areas, using the city name `total` (eg. `Punjab_total`).

Let's clean those out along the way. We'll also drop older years for the (few) cities that have data for multiple years.

```{r}
#| label: process-pt-cities
pt_data |>
  mutate(Cities = na_if(Cities, "NOCITI")) |>
  separate_wider_delim(Cities, "_", names = c("country_iso2", "city") ,too_many = "merge") |>
  mutate(city = str_to_title(city)) |>
  select(country_code = geoAreaCode, country_name = geoAreaName,
    country_iso2, city, year = timePeriodStart, pct_pt_access = value) |>
  # now manually handle a few countries as described above...
  filter(!str_ends(city, "_total")) |>
  mutate(
    city = case_when(
      city == "Washington_d_c" ~ "Washington DC",
      country_name %in% c("Canada", "China", "United States of America") ~
        str_remove(city, "_[^_]*$"),
      country_name == "Mexico" ~
        str_remove(city, "^[^_]*_"),
      country_name == "Pakistan" ~
        str_replace_all(city, c(
          "Khyber_pakhtunkhwa_" = "",
          "Balochistan_" = "",
          "Sindh_" = "",
          "Punjab_" = "")),
      .default = city),
    # ... and switch to title case with spaces
    city = str_replace_all(city, "_", " "),
    city = str_to_title(city)) |>
  # finally, drop non-latest data for a city
  group_by(city) |>
  slice_max(year) |>
  ungroup() ->
pt_tidy
```

I might just double-check my assumption that the apparent iso2 code in the data matches the actual ISO2 code that the UN numeric codes matches.

```{r}
#| label: check-iso
pt_data |>
  filter(Cities != "NOCITI") |>
  separate_wider_delim(Cities, "_", names = c("country_iso2", "city"),too_many = "merge") |>
  mutate(iso2_check = countrycode(geoAreaCode, "un", "iso2c")) |>
  filter(iso2_check != country_iso2) ->
iso2_mismatch

stopifnot(
  "There are countries with bad ISO2 codes provided by the UN SDG API" =
    nrow(iso2_mismatch) == 0)
```

All good!

## Geolocate cities

Finally, we need to geolocate these cities, so we can put them on a map. We'll use [OpenDataSoft's dataset of cities over 1000 people](https://public.opendatasoft.com/explore/dataset/geonames-all-cities-with-a-population-1000), which is CC BY 4.0.

Although OpenDataSoft has an API, to keep things snappy (they have 147k cities and I don't want to query them 20 at a time), I've downloaded the whole thing as a Parquet file.

```{r}
#| label: get-city-locations
here("data", "raw", "cities.csv") |>
  read_delim(delim = ";", na = "") |>
  janitor::clean_names() ->
cities
```

Let's merge them in. The main problem is that the city database is _huge_: there are five entries alone for "Melbourne", including two in the US and two more in the UK.

Since we have the countries for each city, let's use that to assist - we'll do the matching only within country groups. Then, for cases where there are still multiple entries within a city, we'll select the most populous.

Finally, because each city has multiple names, we'll lengthen those out to see if any can be matched.

```{r}
#| label: merge-city-data
cities |>
  # take the most populous record for a city of a given name within a country
  group_by(country_code, ascii_name) |>
  slice_max(population, with_ties = FALSE) |>
  ungroup() |>
  # unspool all the alternate names for each city
  mutate(all_names = paste(name, ascii_name, alternate_names, sep = ",")) |>
  separate_longer_delim(all_names, ",") |>
  select(-ascii_name, -alternate_names) |>
  # finally, eliminate duplicates within a country, as fuzzyjoin has no
  # multiple match handling afaict
  group_by(country_code) |>
  distinct(all_names, .keep_all = TRUE) |>
  ungroup() ->
most_populous_cities

# join the nested city lists and pt access data grouped by country
pt_tidy |>
  nest(pt = -country_iso2) |>
  left_join(
    nest(most_populous_cities, cities = -country_code),
    by = join_by(country_iso2 == country_code)) ->
nested_cities_by_country

# now join the pt and the cities in lockstep
# (this takes a few minutes)
nested_cities_by_country |>
  filter(!is.na(country_iso2)) |>
  mutate(
    matches = map2(pt, cities, stringdist_left_join,
      by = c(city = "all_names"),
      distance_col = "distance",
      .progress = TRUE)) ->
nested_matched_cities

# how many missing matches? about 150 (pretty good!)
nested_matched_cities |>
# nested_jp |>
  select(country_iso2, matches) |>
  unnest(matches) |>
  filter(is.na(coordinates)) |> select(country_iso2, city, all_names) |>
  View()

# now expand back out. since we're keeping all possible matches, this'll
# be huge - we'll keep the closest match in spelling for each city.
nested_matched_cities |>
  select(-pt, -cities) |>
  unnest(matches) |>
  group_by(country_iso2, city) |>
  slice_min(distance) |>
  ungroup() |>
  # now split up coords and make numeric
  separate_wider_delim(coordinates, ", ", names = c("lat", "lon")) |>
  mutate(across(c(lat, lon, pct_pt_access), as.numeric)) |>
  select(geoname_id, country_iso2, country_name, city, city_std = name,
    lat, lon, year, population, pct_pt_access) |>
  filter(!is.na(lat), !is.na(lon)) |>
  write_csv(here("data", "pt-access.csv")) ->
located_cities
```

Let's map them:

```{r}
#| label: map-cities
located_cities |>
  ggplot() +
    aes(x = lon, y = lat) +
    geom_point(aes(colour = pct_pt_access)) +
    scale_colour_fermenter(type = "div", n.breaks = 10, direction = 1) +
    theme_minimal()
```

I would also like to do this as a packed circles by continent, but we'll need to merge in country/continent data from [the UN](https://unstats.un.org/unsd/methodology/m49), but I might swing back 'round to this after the other graphics are done.

# Add continent info

Let's scrape the country listings. Although we could use `rvest::html_table()` for this, the hierarchy of countries, subregions and regions is expressed mostly through html attributes:

- `data-tt-id`: the ID of the row
- `data-tt-parent-id`: the ID of the parent group

Let's first extract these attributes, then get the actual table content as usual with `html_table()` and bolt the two together.

```{r}
#| label: scrape-region-data
doc <- read_html("https://unstats.un.org/unsd/methodology/m49")

doc |>
  html_elements("#GeoGroupsENG tr") |>
  html_attrs() |>
  tail(-1) |>
  map(as_tibble_row) |>
  bind_rows() |>
  set_names(c("id", "parent")) ->
relationships

# now the table itself
doc |>
  html_elements("#GeoGroupsENG") |>
  html_table() |>
  pluck(1) |>
  # bolt the relationships on
  bind_cols(relationships) |>
  janitor::clean_names() |>
  mutate(
    # convert numeric strings to integers
    across(c(parent, id), as.integer),
    # replace blank entries with NA
    across(where(is.character), ~ na_if(.x, ""))) ->
un_groups
```

We have a big list of regions, subregions, and countries (and other areas that have an ISO3 code), plus IDs of their parents. We also have some regions, like Sub-Saharan Africa, that are broken down further into _sub-_ subregions. I'm going to combine those with the subregions so that we just categorise all countries into the broadest regions.

```{r}
#| label: arrange-region-hierarchy
regions <- un_groups |> filter(parent == 1)

# join subregions to regions
un_groups |>
  filter(parent %in% regions$id) |>
  left_join(regions,
    join_by(parent == id),
    relationship = "many-to-one",
    suffix = c("", "_region")) |>
    glimpse() ->
subregions

# any places that are sub-subregions?
un_groups |>
  filter(parent %in% subregions$id, is.na(iso_alpha3_code)) |>
  left_join(subregions,
    join_by(parent == id),
    relationship = "many-to-one",
    suffix = c("", "_subregion")) |>
    glimpse() ->
subsubregions

# bolt subregions and subsubregions together so we can still group
subsubregions |>
  select(country_or_area, m49_code, iso_alpha3_code, other_groupings,
    id, parent = parent_subregion, country_or_area_region, m49_code_region, iso_alpha3_code_region, other_groupings_region,
    parent_region) |>
  bind_rows(subregions) ->
all_subregions

# now for the countries, join to subregions (bringing regions along too)
# i'll also use {countrycode} to convert the iso3 codes to iso2 codes
un_groups |>
  filter(!is.na(iso_alpha3_code)) |>
  left_join(all_subregions,
    join_by(parent == id),
    relationship = "many-to-one",
    suffix = c("", "_subregion")) |>
  mutate(iso2 = countrycode(iso_alpha3_code, "iso3c", "iso2c")) |>
  select(id,
    region = country_or_area_region,
    subregion = country_or_area_subregion,
    iso2,
    iso3 = iso_alpha3_code,
    name = country_or_area) ->
country_region_maps
```

Now we're ready to join this back to our city database and write back out to disk:

```{r}
#| label: join-pt-and-regions
located_cities |>
  left_join(country_region_maps, join_by(country_iso2 == iso2)) |>
  select(-iso3, -name, -id) |>
  select(where(is.character), where(is.numeric)) |>
  write_csv(here("data", "pt-access.csv")) ->
located_cities_regions
  
# also do an asia-pac one
located_cities |>
  left_join(country_region_maps, join_by(country_iso2 == iso2)) |>
  select(-name, -id) |>
  select(where(is.character), where(is.numeric)) |>
  filter(iso3 %in% c(
    "CHN", "HKG", "MAC", "PRK", "JPN", "MNG", "KOR", "BRN", "KHM", "IDN", "LAO", "MYS", "MMR", "PHL", "SGP", "THA", "TLS", "VNM", "AFG", "BGD", "BTN", "IND", "IRN", "MDV", "NPL", "PAK", "LKA", "AUS", "CXR", "CCK", "NZL", "NFK")) |>
  select(-iso3) |>
  write_csv(here("data", "pt-access-ap.csv"))
``` 
